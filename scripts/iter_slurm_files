#!/usr/bin/env python

import argparse
import datetime
import glob
import re
import subprocess
import sys
from pathlib import Path


def get_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Iterates slurm files")
    parser.add_argument("-d", "--slurm-dir", default="~/slurm_logs",
                        help="Directory of slurm files")
    parser.add_argument("--cache-max-hours", default=1, type=int,
                        help="Max cache time for persisting")
    parser.add_argument("-f", "--job-format", default="job-*.err",
                        help="Format for job files")
    parser.add_argument("-s", "--min-file-size", default=300, type=int,
                        help="Minimum file size, in bytes")
    parser.add_argument("-j", "--start-job-id", default=0, type=int,
                        help="Minimum job ID")
    parser.add_argument("-n", "--num-job-ids", default=100000000, type=int,
                        help="Number of job IDs, starting from start ID")
    return parser.parse_args()


def should_renew_cache(f: Path, hours: int) -> bool:
    if not f.exists():
        return True
    mtime = datetime.datetime.fromtimestamp(f.stat().st_mtime)
    if (datetime.datetime.now() - mtime) > datetime.timedelta(hours=hours):
        return True
    return False

def main() -> None:
    args = get_args()

    slurm_dir = Path(args.slurm_dir).expanduser()
    if not slurm_dir.is_dir():
        raise ValueError(f"Slurm directory not found: {slurm_dir}")

    cache_file = slurm_dir / ".slurm_file_cache"
    if should_renew_cache(cache_file, args.cache_max_hours):
        with open(cache_file, "w") as f:
            glob_str = str(slurm_dir / args.job_format)
            print(f"Listing files here: {glob_str}", file=sys.stderr)
            files = glob.glob(str(glob_str))
            files = sorted(files, reverse=True)
            for fname in files:
                fsize = Path(fname).stat().st_size
                print(f"{fname}\t{fsize}", file=f)
        print(f"Wrote cache file to {cache_file}", file=sys.stderr)

    with open(cache_file, "r") as f:
        for line in f:
            fpath_str, fsize = line.strip().split("\t")
            fpath, fsize = Path(fpath_str), int(fsize)
            if not fpath.exists():
                print(f"File no longer exists: {fpath}", file=sys.stderr)
                continue
            if fsize < args.min_file_size:
                print(f"File too small: {fpath}", file=sys.stderr)
                continue
            job_id = re.findall(r"\d+", fpath.stem)
            if not job_id:
                print(f"Couldn't parse job ID: {fpath.stem}")
                continue
            job_id = int(job_id[0])
            start, end = args.start_job_id, args.start_job_id + args.num_job_ids
            if job_id < start or job_id > end:
                print(f"Skipping job {job_id}")
                continue
            p = subprocess.Popen(["less", fpath])
            p.wait()

    print("Done!", file=sys.stderr)

if __name__ == "__main__":
    main()

